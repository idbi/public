///////////////////////////////////////////////////////////////////////////////
// Grafana Alloy Configuration
///////////////////////////////////////////////////////////////////////////////

///////////////////////////////////////////////////////////////////////////////
// Exporters
///////////////////////////////////////////////////////////////////////////////
prometheus.remote_write "default" {
  endpoint {
    url = "https://telemetry.idbi.pe/api/metrics/push"
    headers = {
      "X-Scope-OrgID" = "idbi.pe",
    }
  }
}

otelcol.exporter.prometheus "default" {
  forward_to = [prometheus.remote_write.default.receiver]
}

otelcol.receiver.prometheus "default" {
  output {
    metrics = [otelcol.processor.resourcedetection.default.input]
  }
}

loki.write "default" {
  endpoint {
    url = "https://telemetry.idbi.pe/api/logs/push"
    headers = {
      "X-Scope-OrgID" = "idbi.pe",
    }
  }
}

otelcol.exporter.loki "default" {
  forward_to = [loki.process.extract_all.receiver]
}


otelcol.receiver.loki "default" {
  output {
    logs = [otelcol.processor.resourcedetection.default.input]
  }
}


otelcol.exporter.otlphttp "default" {
  client {
    endpoint = "https://telemetry.idbi.pe:443"
    headers = {
      "X-Scope-OrgID" = "idbi.pe",
    }
  }

  sending_queue {
    enabled = true
    queue_size = 10000  // bigger buffer for initial burst
  }
}


///////////////////////////////////////////////////////////////////////////////
// Resource Detection
///////////////////////////////////////////////////////////////////////////////
otelcol.processor.resourcedetection "default" {
  detectors = ["azure", "system", "env"]
  timeout   = "5s"

  system {
    hostname_sources = ["os", "cname", "dns"]

    resource_attributes {
      host.ip  { enabled = true }
    }
  }

  output {
    logs    = [otelcol.processor.batch.burst_control.input]
    metrics = [otelcol.processor.batch.burst_control.input]
    traces  = [otelcol.processor.batch.burst_control.input]
  }
}

///////////////////////////////////////////////////////////////////////////////
// Processor
///////////////////////////////////////////////////////////////////////////////
otelcol.processor.batch "burst_control" {
  send_batch_size = 1000   // controls per-request size
  send_batch_max_size = 0  // actual batch size may exceed 1000
  timeout = "1s"           // sends more frequently, smaller batches

  output {
    logs    = [otelcol.exporter.loki.default.input]
    metrics = [otelcol.processor.transform.default.input]
    traces  = [otelcol.exporter.otlphttp.default.input]
  }
}


loki.process "extract_all" {
  forward_to = [loki.write.default.receiver]
  stage.json {
    expressions = {
      body                 = "body",
      file                 = "attributes.\"log.file.name\"",
      path                 = "attributes.\"log.file.path\"",
      azure_resourcegroup  = "resources.\"azure.resourcegroup.name\"",
      azure_vm             = "resources.\"azure.vm.name\"",
      azure_scaleset       = "resources.\"azure.vm.scaleset.name\"",
      azure_vm_size        = "resources.\"azure.vm.size\"",
      cloud_account_id     = "resources.\"cloud.account.id\"",
      cloud_platform       = "resources.\"cloud.platform\"",
      cloud_provider       = "resources.\"cloud.provider\"",
      cloud_region         = "resources.\"cloud.region\"",
      host_id              = "resources.\"host.id\"",
      host_name            = "resources.\"host.name\"",
      host_ip              = "resources.\"host.ip\"",
      os_type              = "resources.\"os.type\"",
      telemetry_sdk        = "resources.\"telemetry.sdk.name\"",
    }
  }

  stage.regex {
    expression = `(?P<ipv4>\d{1,3}(?:\.\d{1,3}){3})`
    source = "host_ip"
  }

  stage.regex {
    expression = `(?P<web_ts>\d{2}\/[A-Za-z]{3}\/\d{4}:\d{2}:\d{2}:\d{2} [+-]\d{4})`
    source = "body"
  }

  stage.regex {
    expression = `(?P<go_ts>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:[+-]\d{2}:\d{2}|Z))`
    source = "body"
  }

  stage.labels {
    values = {
      azure_resourcegroup  = "azure_resourcegroup",
      azure_vm             = "azure_vm",
      azure_scaleset       = "azure_scaleset",
      azure_vm_size        = "azure_vm_size",
      cloud_provider       = "cloud_provider",
      cloud_region         = "cloud_region",
      host_id              = "host_id",
      host_ip              = "ipv4",
      host_name            = "host_name",
    }
  }

  stage.timestamp {
    source            = "web_ts"
    format            = "02/Jan/2006:15:04:05 -0700"
    action_on_failure = "skip"
  }

  stage.timestamp {
    source            = "go_ts"
    format            = "2006-01-02T15:04:05Z07:00"
    action_on_failure = "skip"
  }

  stage.output {
    source = "body"
  }

  stage.decolorize {}
}

otelcol.processor.transform "default" {
  error_mode = "ignore"

  metric_statements {
    context = "datapoint"

    statements = [
      `set(datapoint.attributes["azure_resourcegroup"], resource.attributes["azure.resourcegroup.name"])`,
      `set(datapoint.attributes["azure_vm"], resource.attributes["azure.vm.name"])`,
      `set(datapoint.attributes["azure_scaleset"], resource.attributes["azure.vm.scaleset.name"])`,
      `set(datapoint.attributes["azure_vm_size"], resource.attributes["azure.vm.size"])`,
      `set(datapoint.attributes["cloud_provider"], resource.attributes["cloud.provider"])`,
      `set(datapoint.attributes["cloud_region"], resource.attributes["cloud.region"])`,
      `set(datapoint.attributes["host_id"], resource.attributes["host.id"])`,
      `set(datapoint.attributes["host_ip"], resource.attributes["host.ip"][0])`,
      `set(datapoint.attributes["host_name"], resource.attributes["host.name"])`,
    ]
  }

  output {
    metrics = [otelcol.exporter.prometheus.default.input]
  }
}

///////////////////////////////////////////////////////////////////////////////
// Scrappers
///////////////////////////////////////////////////////////////////////////////
// This block relabels metrics coming from node_exporter to add standard labels
discovery.relabel "integrations_node_exporter" {
  targets = prometheus.exporter.unix.integrations_node_exporter.targets

  rule {
    // Set the instance label to the hostname of the machine
    target_label = "instance"
    replacement  = constants.hostname
  }

  rule {
    // Set a standard job name for all node_exporter metrics
    target_label = "job"
    replacement = "integrations/node_exporter"
  }
}

// Configure the node_exporter integration to collect system metrics
prometheus.exporter.unix "integrations_node_exporter" {

  enable_collectors = [
    "processes",
    "sysctl",
  ]

  filesystem {
    // Exclude filesystem types that aren't relevant for monitoring
    fs_types_exclude     = "^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|tmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$"
    // Exclude mount points that aren't relevant for monitoring
    mount_points_exclude = "^/(dev|proc|run/credentials/.+|sys|var/lib/docker/.+)($|/)"
    // Timeout for filesystem operations
    mount_timeout        = "5s"
  }

  netclass {
    // Ignore virtual and container network interfaces
    ignored_devices = "^(veth.*|cali.*|[a-f0-9]{15})$"
  }

  netdev {
    // Exclude virtual and container network interfaces from device metrics
    device_exclude = "^(veth.*|cali.*|[a-f0-9]{15})$"
  }

}

// Define how to scrape metrics from the node_exporter
prometheus.scrape "integrations_node_exporter" {
scrape_interval = "15s"
  // Use the targets with labels from the discovery.relabel component
  targets    = discovery.relabel.integrations_node_exporter.output
  // Send the scraped metrics to the relabeling component
  forward_to = [otelcol.receiver.prometheus.default.receiver]
}


// Collect logs from systemd journal for node_exporter integration
loki.source.journal "logs_integrations_integrations_node_exporter_journal_scrape" {
  // Only collect logs from the last 24 hours
  max_age       = "24h0m0s"
  // Apply relabeling rules to the logs
  relabel_rules = discovery.relabel.logs_integrations_integrations_node_exporter_journal_scrape.rules
  // Send logs to the local Loki instance
  forward_to    = [otelcol.receiver.loki.default.receiver]
}

// Define which log files to collect for node_exporter
local.file_match "logs_integrations_integrations_node_exporter_direct_scrape" {
  path_targets = [{
    // Target localhost for log collection
    __address__ = "localhost",
    // Collect standard system logs
    __path__    = "/var/log/{syslog,messages,*.log,**/*.log}",
    // Add instance label with hostname
    instance    = constants.hostname,
    // Add job label for logs
    job         = "integrations/node_exporter",
  }]
}

// Define relabeling rules for systemd journal logs
discovery.relabel "logs_integrations_integrations_node_exporter_journal_scrape" {
  targets = []

  rule {
    // Extract systemd unit information into a label
    source_labels = ["__journal__systemd_unit"]
    target_label  = "unit"
  }

  rule {
    // Extract boot ID information into a label
    source_labels = ["__journal__boot_id"]
    target_label  = "boot_id"
  }

  rule {
    // Extract transport information into a label
    source_labels = ["__journal__transport"]
    target_label  = "transport"
  }

  rule {
    // Extract log priority into a level label
    source_labels = ["__journal_priority_keyword"]
    target_label  = "level"
  }

  rule {
    // Set the instance label to the hostname of the machine
    target_label = "instance"
    replacement  = constants.hostname
  }

  rule {
    // Set a standard job name for all node_exporter metrics
    target_label = "job"
    replacement = "integrations/node_exporter"
  }
}

// Collect logs from files for node_exporter
loki.source.file "logs_integrations_integrations_node_exporter_direct_scrape" {
  // Use targets defined in local.file_match
  targets    = local.file_match.logs_integrations_integrations_node_exporter_direct_scrape.targets
  // Send logs to the local Loki instance
  forward_to = [otelcol.receiver.loki.default.receiver]
}
